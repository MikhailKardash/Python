# Project 1 Generative Text

Mikhail Kardash, mkardash@ucsd.edu

## Abstract

Wikipedia is an online encyclopedia and is used as a source of information by many around the world. 
Articles exist for many different subjects by different volunteers. 
I trained a GRU network on the text data of these articles out of curiosity. 
How would the neural network learn the writing quirks of the various contributors? 
What would the subject matter of the article generated by the network be? 
I used the wikipedia API to scrape random wikipedia articles.
I used the GRU function with batch sizes of 128 characters, ReLU activations, .1 dropout, and .15 recurrent dropout. Everything else was default.
I also used the Adam Optimizer with 0.0005 learning rate, 0.8 beta1, 0.9 beta2, and 1e-04 epsilon. These were found using trial and error.
Final loss is 1.06 after 80 iterations. The seed is the string "Kapital".

## Files

*scraper.py* scrapes wikipedia webpages.

*summaries.txt* is the used training data.

*out.txt* is the generated output.

*DeepReference.ipynb* is the code for the network.

## Results

It seems that the network generated a tv show called Corporate Cumming.
After generating a season, the network generates the reference category, external links, experiences, and sources.
These titles are in a somewhat logical order but the content contained in them does not relate to the title of each section.
In the see also section, a biography is posted, and that biography also has a references section.
I shall hereby name this network DeepReference.

## Notes

1. scraper.py requires wikipedia package.

2. Training must be done in multiple tries, for some reason first 27 iterations take up a lot of memory but if the kernel is cleared and training is resumed after, there are no memory issues.

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "#import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1100289 characters\n"
     ]
    }
   ],
   "source": [
    "#path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "\n",
    "path_to_file = \"summaries.txt\"\n",
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "text = text.encode('ascii', errors = 'ignore')\n",
    "text = text.decode('ascii')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The European Patent Judges' Symposium (French: Colloque des juges europens de brevets, German: Symposium europischer Patentrichter) is a biennial symposium, with the claimed aim of providing a platform for national judges from legal systems with diff\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\t':   0,\n",
      "  '\\n':   1,\n",
      "  '\\r':   2,\n",
      "  ' ' :   3,\n",
      "  '!' :   4,\n",
      "  '\"' :   5,\n",
      "  '#' :   6,\n",
      "  '$' :   7,\n",
      "  '%' :   8,\n",
      "  '&' :   9,\n",
      "  \"'\" :  10,\n",
      "  '(' :  11,\n",
      "  ')' :  12,\n",
      "  '*' :  13,\n",
      "  '+' :  14,\n",
      "  ',' :  15,\n",
      "  '-' :  16,\n",
      "  '.' :  17,\n",
      "  '/' :  18,\n",
      "  '0' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "\n",
    "\n",
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'The European ' ---- characters mapped to int ---- > [53 70 67  3 38 83 80 77 78 67 63 76  3]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T\n",
      "h\n",
      "e\n",
      " \n",
      "E\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "\n",
    "\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The European Patent Judges' Symposium (French: Colloque des juges europens de brevets, German: Sympos\"\n",
      "'ium europischer Patentrichter) is a biennial symposium, with the claimed aim of providing a platform '\n",
      "'for national judges from legal systems with differing traditions to exchange experiences and to there'\n",
      "'by promote mutual understanding in the development of European patent law.Along with the Standing Adv'\n",
      "'isory Committee before the European Patent Office (SACEPO), a committee advising the European Patent '\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  \"The European Patent Judges' Symposium (French: Colloque des juges europens de brevets, German: Sympo\"\n",
      "Target data: \"he European Patent Judges' Symposium (French: Colloque des juges europens de brevets, German: Sympos\"\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 53 ('T')\n",
      "  expected output: 70 ('h')\n",
      "Step    1\n",
      "  input: 70 ('h')\n",
      "  expected output: 67 ('e')\n",
      "Step    2\n",
      "  input: 67 ('e')\n",
      "  expected output: 3 (' ')\n",
      "Step    3\n",
      "  input: 3 (' ')\n",
      "  expected output: 38 ('E')\n",
      "Step    4\n",
      "  input: 38 ('E')\n",
      "  expected output: 83 ('u')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((128, 100), (128, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size \n",
    "BATCH_SIZE = 128\n",
    "steps_per_epoch = examples_per_epoch//BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences, \n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of GRU units\n",
    "rnn_units = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "rnn = functools.partial(\n",
    "    tf.keras.layers.GRU,\n",
    "    activation = 'relu', \n",
    "    recurrent_activation='sigmoid',\n",
    "    kernel_initializer='glorot_uniform',\n",
    "    recurrent_initializer='glorot_uniform',\n",
    "    bias_initializer='zeros',\n",
    "    dropout = 0.1,\n",
    "    recurrent_dropout = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size,embedding_dim,\n",
    "                                 batch_input_shape=[batch_size,None]),\n",
    "        rnn(rnn_units,\n",
    "           return_sequences = True,\n",
    "           stateful=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim = embedding_dim,\n",
    "    rnn_units = rnn_units,\n",
    "    batch_size = BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 100, 90) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1): \n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (128, None, 256)          23040     \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (128, None, 2048)         14161920  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (128, None, 90)           184410    \n",
      "=================================================================\n",
      "Total params: 14,369,370\n",
      "Trainable params: 14,369,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.multinomial(example_batch_predictions[0], num_samples=1) \n",
    "#sample_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([67, 81, 19, 31, 78, 46, 26, 50, 56, 39, 36, 65, 40, 74, 73, 59, 23,\n",
       "       83, 68, 66, 57, 59, 28, 77, 17, 65, 63, 26, 72, 78, 25, 27, 48, 83,\n",
       "       57, 14, 27, 40, 70, 18, 22, 79, 56, 23, 78, 63, 81,  1, 78, 51, 31,\n",
       "       60, 16, 86, 47, 30, 27, 57, 69, 35, 44, 52, 69, 29, 12,  6, 73, 61,\n",
       "       80, 74, 80, 79, 73, 30, 41, 75,  1, 77, 73, 31, 79, 51, 65, 71, 40,\n",
       "       87, 23, 36,  2, 79, 30, 39,  9, 15, 81, 69, 80, 37, 13, 46])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'oet.\"  In his treatise, De perfecta poesi, he not only wrote that a poet \"invents,\" \"after a fashion'\n",
      "\n",
      "Next Char Predictions: \n",
      " 'es0=pM7QWFCcGlkZ4ufdXZ9o.ca7jp68OuX+8Gh/3qW4pas\\npR=[-xN;8XgBKSg:)#k]rlrqk;Hm\\nok=qRciGy4C\\rq;F&,sgrD*M'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (128, 100, 90)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.498675\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "#   return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "  return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.0005, beta1= 0.8,\n",
    "                                      beta2=0.9,epsilon = 1e-04),\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './text100'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#22 seems to be the limit\n",
    "EPOCHS=27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7f7b22490550>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#'./text100/ckpt_27'\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/27\n",
      "85/85 [==============================] - 74s 866ms/step - loss: 1.2264\n",
      "Epoch 2/27\n",
      "85/85 [==============================] - 75s 884ms/step - loss: 1.2181\n",
      "Epoch 3/27\n",
      "85/85 [==============================] - 75s 881ms/step - loss: 1.2118\n",
      "Epoch 4/27\n",
      "85/85 [==============================] - 74s 872ms/step - loss: 1.2025\n",
      "Epoch 5/27\n",
      "85/85 [==============================] - 75s 882ms/step - loss: 1.1963\n",
      "Epoch 6/27\n",
      "85/85 [==============================] - 76s 891ms/step - loss: 1.1885\n",
      "Epoch 7/27\n",
      "85/85 [==============================] - 80s 938ms/step - loss: 1.1827\n",
      "Epoch 8/27\n",
      "85/85 [==============================] - 73s 862ms/step - loss: 1.1754\n",
      "Epoch 9/27\n",
      "85/85 [==============================] - 75s 884ms/step - loss: 1.1678\n",
      "Epoch 10/27\n",
      "85/85 [==============================] - 77s 908ms/step - loss: 1.1610\n",
      "Epoch 11/27\n",
      "85/85 [==============================] - 73s 855ms/step - loss: 1.1543\n",
      "Epoch 12/27\n",
      "85/85 [==============================] - 73s 861ms/step - loss: 1.1480\n",
      "Epoch 13/27\n",
      "85/85 [==============================] - 75s 877ms/step - loss: 1.1422\n",
      "Epoch 14/27\n",
      "85/85 [==============================] - 73s 853ms/step - loss: 1.1348\n",
      "Epoch 15/27\n",
      "85/85 [==============================] - 73s 863ms/step - loss: 1.1285\n",
      "Epoch 16/27\n",
      "85/85 [==============================] - 74s 865ms/step - loss: 1.1228\n",
      "Epoch 17/27\n",
      "85/85 [==============================] - 72s 847ms/step - loss: 1.1160\n",
      "Epoch 18/27\n",
      "85/85 [==============================] - 74s 866ms/step - loss: 1.1112\n",
      "Epoch 19/27\n",
      "85/85 [==============================] - 74s 868ms/step - loss: 1.1042\n",
      "Epoch 20/27\n",
      "85/85 [==============================] - 75s 885ms/step - loss: 1.0997\n",
      "Epoch 21/27\n",
      "85/85 [==============================] - 75s 877ms/step - loss: 1.0938\n",
      "Epoch 22/27\n",
      "85/85 [==============================] - 74s 869ms/step - loss: 1.0887\n",
      "Epoch 23/27\n",
      "85/85 [==============================] - 73s 857ms/step - loss: 1.0826\n",
      "Epoch 24/27\n",
      "85/85 [==============================] - 73s 865ms/step - loss: 1.0770\n",
      "Epoch 25/27\n",
      "85/85 [==============================] - 74s 876ms/step - loss: 1.0717\n",
      "Epoch 26/27\n",
      "85/85 [==============================] - 74s 873ms/step - loss: 1.0673\n",
      "Epoch 27/27\n",
      "85/85 [==============================] - 73s 854ms/step - loss: 1.0604\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset.repeat(), epochs=EPOCHS, steps_per_epoch=steps_per_epoch, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./text100/ckpt_27'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (1, None, 256)            23040     \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (1, None, 2048)           14161920  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (1, None, 90)             184410    \n",
      "=================================================================\n",
      "Total params: 14,369,370\n",
      "Trainable params: 14,369,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 5000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing) \n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = 1.0\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a multinomial distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "      \n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "      \n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(generate_text(model, start_string=u\"ROMEO\"))\n",
    "#print(generate_text(model, start_string=u\"uwu\")[:300])\n",
    "fileID = open(\"out.txt\",\"w\",encoding = \"utf-8\")\n",
    "fileID.write(generate_text(model,start_string=u\"Kapital\"))\n",
    "fileID.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
